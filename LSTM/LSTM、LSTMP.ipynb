{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d83efca",
   "metadata": {},
   "source": [
    "# <font color = 'red'> LSTM 单元状态</font>\n",
    "![title](pic/LSTM状态单元.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8811a",
   "metadata": {},
   "source": [
    "# <font color = 'red'>LSTM计算公式<font>\n",
    "![title](pic/formula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1963caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.使用官方API代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a38b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0727, -0.1694, -0.0609, -0.0027, -0.1365],\n",
      "         [ 0.1767, -0.2762, -0.1179,  0.0175, -0.2837],\n",
      "         [ 0.0286, -0.3371, -0.2826,  0.2411, -0.2677]],\n",
      "\n",
      "        [[ 0.0685, -0.2497, -0.1915,  0.2847,  0.0358],\n",
      "         [ 0.3631, -0.0035,  0.3106,  0.0022,  0.0099],\n",
      "         [ 0.3331, -0.1415,  0.1982,  0.0215, -0.1402]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "input = torch.randn(bs,T,i_size)\n",
    "c0 = torch.randn(bs,h_size)\n",
    "h0 = torch.randn(bs,h_size)\n",
    "\n",
    "lstm_layer = nn.LSTM(input_size=i_size,hidden_size=h_size,batch_first=True)\n",
    "output,(h_final,c_final) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0)))\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769d1e8",
   "metadata": {},
   "source": [
    "# 2.自己写一个LSTM模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33016edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(input,initial_states,w_ih,w_hh,b_ih,b_hh):\n",
    "    (h0,c0) = initial_states\n",
    "    bs,T,input_size = input.shape\n",
    "    h_size = int(w_ih.shape[0]//4)\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    output_size = h_size\n",
    "    output = torch.randn(bs,T,output_size)\n",
    "\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1) #[bs, 4*h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1) #[bs, 4*h_size, h_size]\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :] # [batch_size,i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))\n",
    "        w_times_x = w_times_x.squeeze(-1) # [bs 4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) # [bs 4*h_size]\n",
    "\n",
    "        i_t = torch.sigmoid(w_times_x[:,:h_size]+w_times_h_prev[:,:h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size] + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:, 2*h_size:3*h_size] + b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3 *h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3 * h_size : 4 * h_size] + w_times_h_prev[:, 3 * h_size:4 * h_size] + b_ih[3 * h_size:4 * h_size] + b_hh[3*h_size:4 * h_size])\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output,(prev_h,prev_c)\n",
    "\n",
    "\n",
    "\n",
    "output_custom,(h_custom,c_custom) = lstm_forward(input,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174c366",
   "metadata": {},
   "source": [
    "# 3.打印对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7d9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0727, -0.1694, -0.0609, -0.0027, -0.1365],\n",
      "         [ 0.1767, -0.2762, -0.1179,  0.0175, -0.2837],\n",
      "         [ 0.0286, -0.3371, -0.2826,  0.2411, -0.2677]],\n",
      "\n",
      "        [[ 0.0685, -0.2497, -0.1915,  0.2847,  0.0358],\n",
      "         [ 0.3631, -0.0035,  0.3106,  0.0022,  0.0099],\n",
      "         [ 0.3331, -0.1415,  0.1982,  0.0215, -0.1402]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.0727, -0.1694, -0.0609, -0.0027, -0.1365],\n",
      "         [ 0.1767, -0.2762, -0.1179,  0.0175, -0.2837],\n",
      "         [ 0.0286, -0.3371, -0.2826,  0.2411, -0.2677]],\n",
      "\n",
      "        [[ 0.0685, -0.2497, -0.1915,  0.2847,  0.0358],\n",
      "         [ 0.3631, -0.0035,  0.3106,  0.0022,  0.0099],\n",
      "         [ 0.3331, -0.1415,  0.1982,  0.0215, -0.1402]]], grad_fn=<CopySlices>)\n",
      "------------------------------------------------------------------------------------------------\n",
      "tensor([[[ 0.0286, -0.3371, -0.2826,  0.2411, -0.2677],\n",
      "         [ 0.3331, -0.1415,  0.1982,  0.0215, -0.1402]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[ 0.0286, -0.3371, -0.2826,  0.2411, -0.2677],\n",
      "        [ 0.3331, -0.1415,  0.1982,  0.0215, -0.1402]], grad_fn=<MulBackward0>)\n",
      "------------------------------------------------------------------------------------------------\n",
      "tensor([[[ 0.0936, -0.8035, -0.3711,  0.5657, -1.0101],\n",
      "         [ 0.6217, -0.2252,  0.3359,  0.0634, -0.2538]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[ 0.0936, -0.8035, -0.3711,  0.5657, -1.0101],\n",
      "        [ 0.6217, -0.2252,  0.3359,  0.0634, -0.2538]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(output_custom)\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(h_final)\n",
    "print(h_custom)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(c_final)\n",
    "print(c_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21c498",
   "metadata": {},
   "source": [
    "# <font color = 'red'> LSTMP</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2cd23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n",
      "tensor([[[ 0.0373,  0.1735, -0.0590],\n",
      "         [ 0.0274,  0.0053,  0.0532],\n",
      "         [ 0.0029, -0.0290,  0.1392]],\n",
      "\n",
      "        [[ 0.2182, -0.0308, -0.2166],\n",
      "         [ 0.1487, -0.0724, -0.0629],\n",
      "         [ 0.0803, -0.0844,  0.0629]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.0373,  0.1735, -0.0590],\n",
      "         [ 0.0274,  0.0053,  0.0532],\n",
      "         [ 0.0029, -0.0290,  0.1392]],\n",
      "\n",
      "        [[ 0.2182, -0.0308, -0.2166],\n",
      "         [ 0.1487, -0.0724, -0.0629],\n",
      "         [ 0.0803, -0.0844,  0.0629]]], grad_fn=<CopySlices>)\n",
      "------------------------\n",
      "tensor([[[ 0.0029, -0.0290,  0.1392],\n",
      "         [ 0.0803, -0.0844,  0.0629]]], grad_fn=<StackBackward0>)\n",
      "tensor([[ 0.0029, -0.0290,  0.1392],\n",
      "        [ 0.0803, -0.0844,  0.0629]], grad_fn=<SqueezeBackward1>)\n",
      "------------------------\n",
      "tensor([[[ 0.3104, -0.5177, -0.3693,  0.2057, -0.0402],\n",
      "         [-0.3326, -0.5337, -0.5554, -0.0329,  0.0275]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[ 0.3104, -0.5177, -0.3693,  0.2057, -0.0402],\n",
      "        [-0.3326, -0.5337, -0.5554, -0.0329,  0.0275]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 1.官方api\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "proj_size = 3\n",
    "input = torch.randn(bs,T,i_size)\n",
    "c0 = torch.randn(bs,h_size)\n",
    "h0 = torch.randn(bs,proj_size)\n",
    "\n",
    "lstm_layer = nn.LSTM(input_size=i_size,hidden_size=h_size,batch_first=True,proj_size = proj_size)\n",
    "output,(h_final,c_final) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2.自己写一个LSTM模型\n",
    "def lstm_forward(input,initial_states,w_ih,w_hh,b_ih,b_hh,w_hr = None):\n",
    "    (h0,c0) = initial_states\n",
    "    bs,T,input_size = input.shape\n",
    "    h_size = int(w_ih.shape[0]//4)\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1) #[bs, 4*h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1) #[bs, 4*h_size, h_size]\n",
    "\n",
    "    if w_hr is not None:\n",
    "        p_size = w_hr.shape[0]\n",
    "        output_size = p_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(bs, 1, 1)\n",
    "    else:\n",
    "        output_size = h_size\n",
    "\n",
    "    output = torch.zeros(bs, T, output_size)\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :] # [batch_size,i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))\n",
    "        w_times_x = w_times_x.squeeze(-1) # [bs 4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) # [bs 4*h_size]\n",
    "\n",
    "        i_t = torch.sigmoid(w_times_x[:,:h_size]+w_times_h_prev[:,:h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size] + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:, 2*h_size:3*h_size] + b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3 *h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3 * h_size : 4 * h_size] + w_times_h_prev[:, 3 * h_size:4 * h_size] + b_ih[3 * h_size:4 * h_size] + b_hh[3*h_size:4 * h_size])\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        if w_hr is not None:\n",
    "            prev_h = torch.bmm(batch_w_hr,prev_h.unsqueeze(-1))\n",
    "            prev_h = prev_h.squeeze(-1)\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output,(prev_h,prev_c)\n",
    "\n",
    "\n",
    "output_custom,(h_custom,c_custom) = lstm_forward(input,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0,lstm_layer.weight_hr_l0)\n",
    "\n",
    "def test():\n",
    "    print(output)\n",
    "    print(output_custom)\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    print(h_final)\n",
    "    print(h_custom)\n",
    "\n",
    "    print(\"------------------------\")\n",
    "    print(c_final)\n",
    "    print(c_custom)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for k,v in lstm_layer.named_parameters():\n",
    "        print(k,v.shape)\n",
    "\n",
    "    test()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e92ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
