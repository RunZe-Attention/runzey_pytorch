{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ca2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8142b",
   "metadata": {},
   "source": [
    "# <font color = 'red'>1、如何基于图片生成patch_embbeding</font>\n",
    "## 方法一\n",
    "- 基于pytorch的unfold API来讲图片进行分块,也就是模仿卷积的思路，设置kernel_size=stride=patch_size,得到分块后的图片\n",
    "- 得到的图片格式为[bs,num_patch,patch_depth]\n",
    "- 将上述张量与形状为[patch_depth,model_dim_C]的张量做线性映射，即可得到[bs,num_patch,model_dim_C]的patch_embbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dcefdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "def image2emb_naive(image,patch_size,weight):\n",
    "    patch = F.unfold(image,kernel_size=(patch_size,patch_size),stride = (patch_size,patch_size)).transpose(-1,-2)\n",
    "    patch_embbeding = patch @ weight\n",
    "    return patch_embbeding\n",
    "\n",
    "\n",
    "image = torch.randn(1,3,8,8)\n",
    "weight = torch.randn(4*4*3,8)\n",
    "\n",
    "print(image2emb_naive(image,4,weight).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393526c",
   "metadata": {},
   "source": [
    "## 方法二\n",
    "- 卷积形式\n",
    "- patch_depth相当于patch_size\\*patch_size\\*input_channel\n",
    "- model_dim_C相当于二维卷积的输出channel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752195ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_convolution(image,kernel,stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride)\n",
    "    bs,oc,oh,ow = conv_output.shape\n",
    "    patch_embbeding = conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n",
    "    return patch_embbeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d47a8",
   "metadata": {},
   "source": [
    "# <font color = 'red'>2、构建MHSA并计算其复杂度</font>\n",
    "- 1.基于输入x[bs,L,C]进行三个映射分别得到q、k、v三个矩阵\n",
    "    - 则每个矩阵$q$、$W$、$v$ 的计算复杂度为$LC^2$,一共是$3LC^2$\n",
    "    \n",
    "- 2.计算attention时候的复杂度\n",
    "    - 1.$q@k^T$的复杂度为$L^2C$\n",
    "    - 2.继续乘以v的复杂度为$L^2C$\n",
    "    - 3.最后做一个线性映射的复杂度为$L2^C$\n",
    "    \n",
    "- 3.虽有MHSA的时间复杂度为\n",
    "    - $4LC^2+2L^2C$\n",
    "   \n",
    "- 4.可以看到传统的MHSA有着与L的平方的关系,具有很高的时间复杂度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf6f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,model_dim,num_head):\n",
    "        super(MultiHeadSelfAttention,self).__init__()\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.proj_linear_layer = nn.Linear(model_dim, 3*model_dim)\n",
    "        self.final_linear_layer = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, input, additive_mask=None):\n",
    "        bs,seq_len,model_dim = input.shape\n",
    "        num_head = self.num_head\n",
    "        head_dim = model_dim // num_head\n",
    "        proj_output = self.proj_linear_layer(input)\n",
    "        q, k, v = proj_output.chunk(3,dim=-1) # shape均为[bs,seq_len,model_dim_C]\n",
    "        # q:[bs,seq_len,model_dim]\n",
    "        q = q.reshape(bs, seq_len, num_head, head_dim).transpose(1, 2).reshape(bs * num_head, seq_len, head_dim)\n",
    "        k = k.reshape(bs, seq_len, num_head, head_dim).transpose(1, 2).reshape(bs * num_head, seq_len, head_dim)\n",
    "        v = v.reshape(bs, seq_len, num_head, head_dim).transpose(1, 2).reshape(bs * num_head, seq_len, head_dim)\n",
    "        if additive_mask is None:\n",
    "            attention_prob = F.softmax(torch.bmm(q,k.transpose(-1,-2))/math.sqrt(head_dim),dim=-1)\n",
    "        else:\n",
    "            additive_mask = additive_mask.tile(num_head,1,1)\n",
    "            attention_prob = F.softmax(torch.bmm(q,k.transpose(-1,-2))/math.sqrt(head_dim)+additive_mask,dim=-1)\n",
    "        output = torch.bmm(attention_prob, v) # [bs*num_head,seq_len,head_dim]\n",
    "        output = output.reshape(bs, num_head, seq_len, head_dim).transpose(1,2)\n",
    "        output = output.reshape(bs, seq_len, model_dim)\n",
    "        output = self.final_linear_layer(output)\n",
    "        return attention_prob, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b44fe1",
   "metadata": {},
   "source": [
    "# <font color = 'red'>3、构建window MHSA 并计算其复杂度</font>\n",
    "\n",
    "- 将patch后的图片进一步分成一个个更大的window\n",
    "    - 1.需要将3D的patch embedding转换为图片的格式\n",
    "    - 2.使用unfold函数将patch划分为window\n",
    "    \n",
    "- 在每个window内部计算MHSA\n",
    "    - window数目可以跟batch_size进行同一对待,因为在window之间没有交互计算\n",
    "    - 关于计算WMHSA的时间复杂度\n",
    "        - 假设窗的边长为W,那么计算每个窗的复杂度为$4W^2C^2+2W^4C$\n",
    "        - 一共有窗的个数为$L/W^2$\n",
    "        - 因此总的复杂度为二者相乘$4LC^2+2LW^2C$\n",
    "    \n",
    "    - 此处不需要mask\n",
    "    - 将计算结果转换成带window的4D tenser\n",
    "    \n",
    " \n",
    "- 复杂度对比\n",
    "    - `MHSA`: $4LC^2+2L^2C$\n",
    "    - `W-MHSA`: $4LC^2+2LW^2C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "736bea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_multi_head_self_attention(patch_embbeding,mhsa,window_size=4,num_head=2):\n",
    "    num_patch_in_window = window_size*window_size\n",
    "    bs, num_patch, patch_depth = patch_embbeding.shape\n",
    "    image_height = image_width = int(math.sqrt(num_patch))\n",
    "\n",
    "    patch_embbeding = patch_embbeding.transpose(-1, -2)\n",
    "    patch = patch_embbeding.reshape(bs,patch_depth, image_height, image_width)\n",
    "    window = F.unfold(patch,kernel_size=(window_size,window_size),stride=(window_size,window_size),).transpose(-1,-2)\n",
    "\n",
    "\n",
    "    bs,num_windows,patch_depth_times_num_patch_in_window = window.shape\n",
    "    window = window.reshape(bs*num_windows,patch_depth,num_patch_in_window).transpose(-1,-2)\n",
    "\n",
    "    attn, output = mhsa(window)# [bs*num_window,num_patch_in_window,PATCH_depth]\n",
    "\n",
    "    output = output.reshape(bs,num_windows,num_patch_in_window,patch_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae8f32",
   "metadata": {},
   "source": [
    "# <font color = 'red'>4、构建shifted Window MHSA 及其MASK</font>\n",
    "- 将上一步的 W-MHSA转换为图片的形式即:[bs,num_windows,num_patch_in_window,patch_depth]转换为[bs,ic,image_h,image_w]\n",
    "- 假设已经做了新的window划分,这一步叫做shift-window\n",
    "- 为了保持window数目不变从而有高效的计算,需要将图片的patch往左和往上各自滑动半个窗口大小的步长,保持patch所属window类别不变\n",
    "- 将图片patch还原成window的数据格式\n",
    "- 由于shift_window之后,每个window虽然形状罪证,但是部分window存在不属于同一个窗口的patch,所以要生成mask\n",
    "- 如何生成mask\n",
    "    - 首先构建一个shift-window的patch所属的window类别矩阵\n",
    "    - 对该矩阵进行同样的王座和网上各自滑动半个窗口大小的步长的操作\n",
    "    - 通过unfold操作可得到[bs,num_window,num_patch_in_window]形状的类别矩阵\n",
    "    - 对该矩阵进行扩维:[bs,num_window,num_patch_in_window,1]\n",
    "    - 该矩阵与自身的转置作差,得到同类关系矩阵，（为0的位置patch的关系属于同类）\n",
    "    - 对同类矩阵中的非0的位置用负无穷进行填充,对于零的位置用0去填充,这样就构建好了MHS所需要的Mask\n",
    "    - 这个mask矩阵的形状为[bs,num_window,num_patch_in_window,num_patch_in_window]\n",
    "- 将window转换为三维的形式:[bs * num_window,num_patch_in _window，patch_size]\n",
    "- 将三维格式的特征连通mask一起送入MHSA中计算得到注意力输出\n",
    "- 将注意力输出换转为图片patch形式:[bs,num_window,num_patch_in_window,patch_size]\n",
    "- 为了恢复位置,需要将图片的patch往右和往左滑动半个window大小\n",
    "- 至此,SW-MHSA计算完成\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63d1850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建SW-MHSA附属函数1:将window形式转换为image形式:目的在于将image进行shift\n",
    "def window2image(msa_output):\n",
    "    bs, num_windows, num_patch_in_windows, patch_depth = msa_output.shape\n",
    "    window_size = int(math.sqrt(num_patch_in_windows))\n",
    "    image_height = image_width = int(math.sqrt(num_windows)) * window_size\n",
    "    msa_output = msa_output.reshape(bs,int(math.sqrt(num_windows)),int(math.sqrt(num_windows)),window_size,window_size,patch_depth)\n",
    "    msa_output = msa_output.transpose(2,3)\n",
    "    image = msa_output.reshape(bs,image_height*image_width,patch_depth)\n",
    "    image = image.transpose(-2,-1)\n",
    "    image = image.reshape(bs,patch_depth,image_height,image_width)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e97cc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建SW-MHSA附属函数2:进行window-shift\n",
    "def shift_window(w_msa_output,window_size,shift_size,generate_mask=False):\n",
    "    bs, num_window, num_patch_in_window,patch_depth = w_msa_output.shape\n",
    "    w_msa_output = window2image(w_msa_output)\n",
    "    bs,patch_depth,image_height,image_weight = w_msa_output.shape\n",
    "    rolled_w_msa_output = torch.roll(w_msa_output,shifts=(shift_size,shift_size),dims=(2,3))\n",
    "    shifted_w_msa_output = rolled_w_msa_output.reshape(bs,patch_depth,int(math.sqrt(num_window)),window_size,int(math.sqrt(num_window)),window_size)\n",
    "    shifted_w_msa_output = shifted_w_msa_output.transpose(3,4)\n",
    "    shifted_w_msa_output = shifted_w_msa_output.reshape(bs,patch_depth,num_window*num_patch_in_window)\n",
    "    shifted_w_msa_output = shifted_w_msa_output.transpose(-1,-2)\n",
    "    shifted_window = shifted_w_msa_output.reshape(bs,num_window,num_patch_in_window,patch_depth)\n",
    "\n",
    "    if generate_mask:\n",
    "        additive_mask = build_mask_for_shifted_wmsa(bs,image_height,image_weight,window_size)\n",
    "    else:\n",
    "        additive_mask = None\n",
    "\n",
    "    return shifted_window,additive_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c306547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建SW-MHSA附属函数3:构建mask矩阵\n",
    "def build_mask_for_shifted_wmsa(batch_size,image_height,image_width,window_size):\n",
    "    index_matrix = torch.zeros(image_height,image_width)\n",
    "    for i in range(image_height):\n",
    "        for j in range(image_width):\n",
    "            row_times = (i+window_size//2) // window_size\n",
    "            col_times = (j+window_size//2) // window_size\n",
    "            index_matrix[i,j] = row_times * (image_height // window_size) + col_times + 1\n",
    "\n",
    "    roll_index_matrix = torch.roll(index_matrix,shifts=(-window_size//2,-window_size//2),dims=(0,1))\n",
    "    roll_index_matrix = roll_index_matrix.unsqueeze(0).unsqueeze(0) #[bs,ch,h,w]\n",
    "\n",
    "    c = F.unfold(roll_index_matrix,kernel_size=(window_size,window_size),stride=(window_size,window_size)).transpose(-1,-2)\n",
    "\n",
    "    c = c.tile(batch_size,1,1)\n",
    "\n",
    "    bs,num_window,num_patch_in_window = c.shape\n",
    "\n",
    "    c1 = c.unsqueeze(-1)\n",
    "    c2 = (c1-c1.transpose(-1,-2))== 0\n",
    "    valid_matrix = c2.to(torch.float32)\n",
    "\n",
    "    unlimit_min = -1e-9\n",
    "    additive_mask = (1-valid_matrix) * unlimit_min\n",
    "\n",
    "    additive_mask = additive_mask.reshape(bs * num_window,num_patch_in_window,num_patch_in_window)\n",
    "    print(additive_mask.shape)\n",
    "    return additive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c5731d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_window_multi_head_self_attention(w_msa_output,mhsa,window_size=4,num_head=2):\n",
    "    bs,num_window,num_patch_in_window,patch_depth = w_msa_output.shape\n",
    "    shifted_w_msa_output ,additive_mask = shift_window(w_msa_output,window_size,\n",
    "                                                       shift_size=-window_size//2,generate_mask=True)\n",
    "\n",
    "    shifted_w_msa_output = shifted_w_msa_output.reshape(bs*num_window,num_patch_in_window,patch_depth)\n",
    "\n",
    "    _, output = mhsa(shifted_w_msa_output,additive_mask)\n",
    "    output = output.reshape(bs, num_window,num_patch_in_window,patch_depth)\n",
    "\n",
    "    output,_ = shift_window(output,window_size,shift_size=window_size//2,generate_mask=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e5718",
   "metadata": {},
   "source": [
    "# <font color = 'red'>5、如何构建PatchMerging</font>\n",
    "- 将window转换为patch\n",
    "- 利用unfold操作,按照merge_size\\*merge_size大小得到新的patch,[bs,num_patch_new,merge_size*merge_size*patch_depth_old]\n",
    "- 增加全连接层对patch_depth进行映射,一般将维度成0.5倍,输出的patch_embbe的形状为[bs,num_patch,patch_depth]\n",
    "- 举例说明:以merge_size=2为例,num_patch变为原来的1/4,patch_depth变为原来的2倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7afe8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def  __init__(self, model_dim, merge_size, output_depth_scale = 0.5):\n",
    "        super(PatchMerging,self).__init__()\n",
    "        self.merge_size = merge_size\n",
    "        self.proj_layer = nn.Linear(\n",
    "            model_dim*merge_size*merge_size,\n",
    "            int(model_dim*merge_size*merge_size*output_depth_scale))\n",
    "\n",
    "    def forward(self,input):\n",
    "        bs,num_window,num_patch_in_window,patch_depth = input.shape\n",
    "        #window_size = int(math.sqrt(num_patch_in_window))\n",
    "        input = window2image(input)  # [bs,patch_depth,image_h,image_w]\n",
    "        merged_window = F.unfold(input,kernel_size=(self.merge_size,self.merge_size),\n",
    "                                 stride=(self.merge_size,self.merge_size)).transpose(-1,-2)\n",
    "\n",
    "        merged_window = self.proj_layer(merged_window)\n",
    "\n",
    "        return merged_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae3be5",
   "metadata": {},
   "source": [
    "# <font color = 'red'>6、构建SwinTransformerBlock</font>\n",
    "- 1、每个block包含有LayerNorm,WMHSA、MLP、SMHSA\n",
    "- 2、输入的是patch_embbeding[bs,num_patch,patch_depth]\n",
    "- 3、其中每个MLP与所有的Transformer族一致，都是包含了两个Linear,第一个Layer将model_dim_C映射到4\\*model_dimC,第二个layer将4\\*model_dim_C映射会model_dim_C维度\n",
    "- 4、输出的维度为[bs,num_patch,num_patch_in_window,patch_depth]\n",
    "- 5、注意残差链接的时候输入与输出的维度需要保持一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e47a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self,model_dim,window_size,num_head):\n",
    "        super(SwinTransformerBlock,self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm4 = nn.LayerNorm(model_dim)\n",
    "\n",
    "        self.wsma_mlp1 = nn.Linear(model_dim,4*model_dim)\n",
    "        self.wsma_mlp2 = nn.Linear(4*model_dim,model_dim)\n",
    "        \n",
    "        self.swsma_mlp1 = nn.Linear(model_dim, 4 * model_dim)\n",
    "        self.swsma_mlp2 = nn.Linear(4 * model_dim, model_dim)\n",
    "\n",
    "        self.mhsa1 = MultiHeadSelfAttention(model_dim,num_head)\n",
    "        self.mhsa2 = MultiHeadSelfAttention(model_dim,num_head)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,num_patch,patch_depth = input.shape\n",
    "        \n",
    "        # WMHSA\n",
    "        input1 = self.layer_norm1(input)\n",
    "        w_msa_output = windows_multi_head_self_attention(input1,self.mhsa1,window_size=4,num_head=2)\n",
    "        bs,num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "        w_msa_output = input + w_msa_output.reshape(bs,num_patch,patch_depth)\n",
    "        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))\n",
    "        output1 += w_msa_output\n",
    "\n",
    "        # SWMHSA\n",
    "        input2 = self.layer_norm3(output1)\n",
    "        input2 = input2.reshape(bs,num_window,num_patch_in_window,patch_depth)\n",
    "        sw_msa_output = shift_window_multi_head_self_attention(input2, self.mhsa2,window_size=4,num_head=2)\n",
    "        sw_msa_output = output1+sw_msa_output.reshape(bs,num_patch,patch_depth)\n",
    "        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))\n",
    "        output2 += sw_msa_output\n",
    "\n",
    "        output2 = output2.reshape(bs,num_window,num_patch_in_window,patch_depth)\n",
    "        return output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1786850",
   "metadata": {},
   "source": [
    "# <font color = 'red'>7、构建SwinTransformerModel</font>\n",
    "- 输入是图片[bs,ic,image_h,image_w]\n",
    "- 首先对图片进行分块得到patch embedding\n",
    "- 根据论文进入四个stage\n",
    "- 对最后一个输出转换为patch embedding的形式[bs,num_patch,patch_depth]\n",
    "- 对patch embbeding进行时间维度的平均池化操作,并映射得到分类的logits,分类完毕\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1740dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerModel(nn.Module):\n",
    "    def __init__(self,input_image_channel = 3,patch_size = 4,model_dim_C = 8,num_classes = 10,window_size = 4,num_head=2,merge_size = 2):\n",
    "        super(SwinTransformerModel,self).__init__()\n",
    "\n",
    "        patch_depth = patch_size*patch_size*input_image_channel\n",
    "        self.patch_size = patch_size\n",
    "        self.model_dim_C = model_dim_C\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.patch_embbeding_weight = nn.Parameter(torch.randn(patch_depth,model_dim_C))\n",
    "\n",
    "        self.block1 = SwinTransformerBlock(model_dim_C,window_size,num_head)\n",
    "        self.block2 = SwinTransformerBlock(model_dim_C*2, window_size, num_head)\n",
    "        self.block3 = SwinTransformerBlock(model_dim_C*4, window_size, num_head)\n",
    "        self.block4 = SwinTransformerBlock(model_dim_C*8, window_size, num_head)\n",
    "\n",
    "        self.patch_merging1 = PatchMerging(model_dim_C,merge_size)\n",
    "        self.patch_merging2 = PatchMerging(model_dim_C*2, merge_size)\n",
    "        self.patch_merging3 = PatchMerging(model_dim_C*4, merge_size)\n",
    "\n",
    "        self.final_linear = nn.Linear(model_dim_C*8,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,image):\n",
    "        patch_embbeding_naive = image2emb_naive(image,self.patch_size,self.patch_embbeding_weight)\n",
    "\n",
    "        # stage1\n",
    "        patch_embbeding = patch_embbeding_naive\n",
    "        print(patch_embbeding.shape)\n",
    "        sw_msa_output = self.block1(patch_embbeding)\n",
    "        print(\"stage1_output:\",sw_msa_output.shape)\n",
    "\n",
    "        # stage2\n",
    "        merged_patch1 = self.patch_merging1(sw_msa_output)\n",
    "        sw_msa_output_1 = self.block2(merged_patch1)\n",
    "        print(\"stage2_output:\", sw_msa_output_1.shape)\n",
    "\n",
    "        # stage3\n",
    "        merged_patch2 = self.patch_merging2(sw_msa_output_1)\n",
    "        sw_msa_output_2 = self.block3(merged_patch2)\n",
    "        print(\"stage3_output:\", sw_msa_output_2.shape)\n",
    "\n",
    "        # stage4\n",
    "        merged_patch3 = self.patch_merging3(sw_msa_output_2)\n",
    "        sw_msa_output_3 = self.block4(merged_patch3)\n",
    "        print(\"stage4_output:\", sw_msa_output_3.shape)\n",
    "\n",
    "        bs,num_window,num_patch_in_window,patch_depth = sw_msa_output_3.shape\n",
    "        sw_msa_output_3 = sw_msa_output_3.reshape(bs,-1,patch_depth)\n",
    "        pool_output = torch.mean(sw_msa_output_3,dim=1)\n",
    "        logits = self.final_linear(pool_output)\n",
    "\n",
    "        print(\"logits\",logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f840b",
   "metadata": {},
   "source": [
    "# <font color = 'red'>8、测试主函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36b6664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4096, 8])\n",
      "torch.Size([1024, 16, 16])\n",
      "stage1_output: torch.Size([4, 256, 16, 8])\n",
      "torch.Size([256, 16, 16])\n",
      "stage2_output: torch.Size([4, 64, 16, 16])\n",
      "torch.Size([64, 16, 16])\n",
      "stage3_output: torch.Size([4, 16, 16, 32])\n",
      "torch.Size([16, 16, 16])\n",
      "stage4_output: torch.Size([4, 4, 16, 64])\n",
      "logits torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bs,ic,image_h,image_w = 4,3,256,256\n",
    "    patch_size = 4\n",
    "    model_dim_C = 8\n",
    "    num_classes = 10\n",
    "    window_size = 4\n",
    "    num_head = 2\n",
    "    merge_size = 2\n",
    "\n",
    "    patch_depth = patch_size*patch_size*ic\n",
    "    image = torch.randn(bs,ic,image_h,image_w)\n",
    "    model = SwinTransformerModel(ic,patch_size,model_dim_C,num_classes,window_size,num_head,merge_size)\n",
    "\n",
    "    model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d18b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
